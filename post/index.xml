<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ernesto Ramirez</title>
    <link>/post/index.xml</link>
    <description>Recent content in Posts on Ernesto Ramirez</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Examining Variability in Physical Activity Data with R</title>
      <link>/post/2017/09/10/examining-variability-in-physical-activity-data-with-r/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/09/10/examining-variability-in-physical-activity-data-with-r/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-eight-visualization-methods&#34;&gt;The Eight Visualization Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-data&#34;&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#daily-steps&#34;&gt;1. Daily Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#phase-mean-and-phase-median-lines&#34;&gt;2. Phase Mean and Phase Median Lines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#daily-average-per-week-and-weekly-median&#34;&gt;3. Daily Average per Week and Weekly Median&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weekly-cumulative&#34;&gt;4. Weekly Cumulative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#proportion-of-baseline-mean-and-proportion-of-baseline-median&#34;&gt;5. Proportion of Baseline Mean and Proportion of Baseline Median&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#day-moving-average&#34;&gt;6. 7-Day Moving Average&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confidence-intervals&#34;&gt;7. Confidence Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#change-point-detection&#34;&gt;8. Change-point Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;As part of my regular work I keep a constant eye on the latest published research that involves the use of Fitbit activity tracking devices. Each new publication is gathered, read (sometimes skimmed), and tagged with appropriate meta-data for our &lt;a href=&#34;https://www.fitabase.com/research-library/&#34;&gt;Fitabase Research Library&lt;/a&gt;. It’s fun work, and it gives me a chance to peak into the every-expanding world of research and clinical use cases for consumer tracking devices and the data they can collect.&lt;/p&gt;
&lt;p&gt;Earlier this summer, while I was traveling to a conference I came across an interesting paper published by a group from the University of South Florida that explored strategies for interpreting highly variable data from long-term use of a Fitbit. I’ll let them explain:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Data Presentation Options to Manage Variability in Physical Activity Research&lt;/em&gt;&lt;br /&gt;
This paper presents seven tactics for managing the variability evident in some physical activity data. High levels of variability in daily step-count data from pedometers or accelerometers can make typical visual inspection difficult. Therefore, the purpose of the current paper is to discuss several strategies that might facilitate the visual interpretation of highly variable data. The seven strategies discussed in this paper are phase mean and median lines, daily average per week, weekly cumulative, proportion of baseline, 7-day moving average, change point detection, and confidence intervals. We apply each strategy to a data set and discuss the advantages and disadvantages. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/jaba.397/abstract&#34;&gt;LINK&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s an interesting paper, with some general, easy-to-use, methods of visualizing daily-scale time series data. While reading it, I got to thinking that all of the methods they explored should be able to be easily reproducible in R. Being a sucker for practicing my R skills I set out to create some reproducible examples of each method with my own Fitbit data. A few cross-country flights later I think I’ve done a decent enough job. Let’s jump in!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;. The data referenced below is available on Github (as you’ll see in the &lt;code&gt;read_csv&lt;/code&gt; call). I invite you to use it so you can follow along with the examples below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-eight-visualization-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Eight Visualization Methods&lt;/h2&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Data&lt;/h3&gt;
&lt;p&gt;In the paper, they use approximately 180 days of daily step data from a Fitbit One that was worn by a participant in a previous study. It’s important to note that this data comes from an intervention study, with three phases: baseline, intervention phase 1, and intervention phase 2. Now, we don’t have access to their participant’s data, so I set out to use my own. Using our Fitabase platform I exported one year of daily step data, from 2016-05-12 to 2017-05-11. I also applied somewhat arbitrary cutoffs for the three phases: 30 days for “baseline”, 135 days for “intervention 1”, and 200 days for “intervention 2”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(scales)
library(ggthemes)
library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data 
# format date when loading in
Daily_Steps &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/erramirez/datascience/master/sevenways/dailySteps_20160512_20170511.csv&amp;quot;, 
   col_types = cols(
    ActivityDay= col_date(&amp;quot;%m/%d/%Y&amp;quot;), 
    StepTotal = col_integer()))

# create a day variable to number the observations by day
# used first day as &amp;quot;1&amp;quot; instead of zero
# add a observation variable with three periods
# Day 1 - 30  = baseline
# Day 31 - 165 = intervention 1
# Day 195 - 365 = intervention 2
startdate &amp;lt;- min(Daily_Steps$ActivityDay)

Daily_Steps &amp;lt;- Daily_Steps %&amp;gt;% 
  mutate(days = as.numeric(difftime(ActivityDay, startdate, units = &amp;quot;days&amp;quot;) + 1),
         observation = as.factor(case_when(days &amp;lt;= 30 ~&amp;quot;Baseline&amp;quot;,
                                           days &amp;lt; 195 ~ &amp;quot;Intervention 1&amp;quot;,
                                           days &amp;lt;= 365 ~ &amp;quot;Intervention 2&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;daily-steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Daily Steps&lt;/h3&gt;
&lt;p&gt;Pretty simple here, let’s just visualize the daily steps taken with days along the &lt;em&gt;x&lt;/em&gt;-axis and total steps on the &lt;em&gt;y&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this plot, and the plots that follow I attempted to use similar formatting to what was presented in the article. They’re pretty minimal so I went with &lt;code&gt;theme_bw&lt;/code&gt; and then made a few adjustments. Originally, I used &lt;code&gt;theme_tufte&lt;/code&gt; from the excellent &lt;a href=&#34;https://github.com/jrnold/ggthemes&#34;&gt;ggthemes package&lt;/a&gt;, but found it’s handling of axis lines a bit restrictive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate a line+points plot to show one year of daily step data
# add some formatting to match journal article 

daily_steps_plot &amp;lt;- ggplot(Daily_Steps, aes(days, StepTotal)) +
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000), limits = c(0,38000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Steps&amp;quot;, 
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(30.5, 195.5)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5), y = 38000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;))

daily_steps_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/dailysteps-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;phase-mean-and-phase-median-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Phase Mean and Phase Median Lines&lt;/h3&gt;
&lt;p&gt;This was also pretty simple. To better understand how the daily steps varied from the mean and median values for each observation period (baseline, intervention 1, intervention 2) we can use &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt; to easily calculate the mean and median and plug those values into &lt;code&gt;geom_segment&lt;/code&gt; in order to plot three separate lines with the correct start and end dates.&lt;/p&gt;
&lt;p&gt;This method of creating an additional data set to use in conjunction with &lt;code&gt;geom_segment&lt;/code&gt; will be useful throughout the remainder to of the visualizations we go over here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create data sets for mean/median grouped by observation period
# also add start and stop days per observation period

observation_data &amp;lt;- Daily_Steps %&amp;gt;% 
  group_by(observation) %&amp;gt;% 
  summarise(mean = mean(StepTotal),
            median = median(StepTotal),
            start = min(days), 
            end = max(days))

# create a plot with mean steps per day per observation period
# uses geom_segment and observation_data to draw mean lines

phase_mean_plot &amp;lt;- ggplot(Daily_Steps, aes(days, StepTotal)) +
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000), limits = c(0,35000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Steps&amp;quot;,
       subtitle = &amp;quot;With Mean Steps per Day per Observation Period&amp;quot;,
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(30, 195)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5),
           y = 34000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = start, y = mean, xend = end, yend = mean), data = observation_data)

phase_mean_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/phase_means-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a plot with median steps per day per observation period
# uses geom_segment and observation_data to draw median lines

phase_median_plot &amp;lt;- ggplot(Daily_Steps, aes(days, StepTotal)) + 
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000), limits = c(0,35000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Steps&amp;quot;,
       subtitle = &amp;quot;With Median Steps per Day per Observation Period&amp;quot;,
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(30, 195)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5),
           y = 34000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = start, y = median, xend = end, yend = median), data = observation_data)
  
phase_median_plot  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/phase_medians-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;daily-average-per-week-and-weekly-median&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Daily Average per Week and Weekly Median&lt;/h3&gt;
&lt;p&gt;Now things start to get interesting! Understanding the variability at longer time scale is a nice way to better understand how activity changes over longer periods of time. In this example, we’re reducing the daily variability to weekly variability by calculating the mean and median steps per day for each week in the data set.&lt;/p&gt;
&lt;p&gt;While this seems easy, there are some choices we have to make here. Primarily, what is a week? Is a calendar week? Is it just seven consecutive days? If it’s a calendar week, does is start or end on Sunday? What happens when a week transverses an study phase transition?&lt;/p&gt;
&lt;p&gt;For ease of creating reusable examples we can actually explore how to derive a “week” using two definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Monday-Sunday seven-day week&lt;/li&gt;
&lt;li&gt;A running seven-day week, regardless of day of the week&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this first method, we use &lt;code&gt;isoweek&lt;/code&gt; from the &lt;code&gt;lubridate&lt;/code&gt; package to find the week number based on a Monday-Sunday week. Keep in mind that our data spans both 2016 and 2017, so the resulting &lt;em&gt;week&lt;/em&gt; variable will repeat and return the same value for the beginning and ending observations since the data doesn’t start on a Monday. I also wanted to create a variable, &lt;em&gt;weeknum&lt;/em&gt;, that reflected the number of weeks elapsed. Last week I was exploring how to calculate streaks in data, and stumbled upon &lt;a href=&#34;https://stackoverflow.com/questions/1502910/how-can-i-count-runs-in-a-sequence&#34;&gt;a great comment stackoverflow&lt;/a&gt; about numbering runs in a series. I applied that here, and started &lt;em&gt;weeknum&lt;/em&gt; at 1 instead of 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create week variable based on isoweek
# create weenum variable for running week number as weeknum restarts at begining of the year
Daily_Steps &amp;lt;- Daily_Steps %&amp;gt;% 
  mutate(week = isoweek(Daily_Steps$ActivityDay),
         weeknum = c(0,cumsum(week[-1L] != week[-length(week)]))+1
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second method, wanted to explore how to set a week as seven consecutive daily observations regardless of the day of the week. I had some old code I used during my dissertation that I was able to re-use here. It’s not pretty and could probably be updated to make use of tidyverse principles, but it works for now. &lt;em&gt;(Tips/ideas on how to improve this section are welcomed!)&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to create data frame of length: 7 observations
weekwindow &amp;lt;- function(x) {
  week &amp;lt;- x[n:(n+6),]
  return(week)
} 

# initialize n to value 1
n &amp;lt;- 1

#create empty list
Daily_Steps_list &amp;lt;- list()

#loop through data and append 7-day moving windows to the list
for (i in 1:53) { #set length of function to 1:number of weeks in the data set
  df &amp;lt;- weekwindow(Daily_Steps) #create a new dataframe based on weekwindow function
  df$dayreg &amp;lt;- c(1:nrow(df)) #create a new variable and set to the day in the 7-day window (1:7)
  df$week &amp;lt;- i #create a new variable and set to the value of i; gives week number
  Daily_Steps_list[[i]] &amp;lt;- df #adds the dataframe (df) to the empty list previously set
  n &amp;lt;- n+7 #increments n by 7 so weeknumber function calls next 7 days. 
} 

# combine all 7-day/week data frames in week.list into a single dataframe
# get rid of trailing NA entries from week 53
Daily_Steps_7DayWeek &amp;lt;- bind_rows(Daily_Steps_list) %&amp;gt;% 
  filter(ActivityDay &amp;lt;= &amp;quot;2017-05-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now, we have two data sets that reflect two different ways to explore weeks in our data. Now comes the fun part - visualizing it! I personally like calendar weeks (isoweek / method one above), as they make conceptual sense when we’re talking about dates so I’ll use that data as the basis for creating the plots. First, we have to create the summarized data set so we have the daily mean, median and standard deviation per week. We do have to keep in mind here that not all weeks will have seven days as observations may have started/ended in the middle of a week (in this data week 1 only has 4 days).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summarize data by week number
# append observation variable to summarized weekly data
# Baseline = Weeks 1-5
# Intervention 1 = Weeks 6-28
# Intervention 2 = Weeks 29-53
Mean_Median_Week &amp;lt;- Daily_Steps %&amp;gt;%  
  group_by(weeknum) %&amp;gt;% 
  summarise(mean = mean(StepTotal),
            median = median(StepTotal),
            sd = sd(StepTotal)) %&amp;gt;% 
  mutate(observation = case_when(weeknum &amp;lt;= 5 ~ &amp;quot;Baseline&amp;quot;,
                                 weeknum &amp;lt;= 28 ~ &amp;quot;Intervention 1&amp;quot;,
                                 weeknum &amp;lt;= 53 ~ &amp;quot;Intervention 2&amp;quot;)
         )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot. We’ll use a good deal of the code we used in the previous plots here. However, note that we’re grouping by the observation so that we have distinct series visualized in the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the mean week with standard deviation error bars
# seperate the oberservation periods using geom_vline
Mean_Week &amp;lt;- ggplot(Mean_Median_Week, aes(weeknum, mean, group = observation)) + 
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd), width = .5) +
  scale_x_continuous(breaks = seq(0,53,1), limits = c(0,53), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,28000,2000), limits = c(0,28000)) +
  #theme_tufte() +
  #geom_rangeframe() +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Average Steps per Week&amp;quot;,
       x = &amp;quot;WEEKS&amp;quot;, 
       y = &amp;quot;AVERAGE DAILY STEPS&amp;quot;,
       caption = &amp;quot;Note: Error bars depict the standard deviation of the mean.&amp;quot;) +
  geom_vline(xintercept = c(5.5, 28.5)) +
  annotate(&amp;quot;text&amp;quot;, x = c(2.25, 16, 42),
           y = 25000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;))

Mean_Week&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/mean_week_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the median step counts per week
Median_Week &amp;lt;- ggplot(Mean_Median_Week, aes(weeknum, median, group = observation)) + 
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(0,53,1), limits = c(0,53), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,19000,2000), limits = c(0,19000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Median Daily Steps per Week&amp;quot;,
       x = &amp;quot;WEEKS&amp;quot;, 
       y = &amp;quot;MEDIAN DAILY STEPS&amp;quot;) +
  geom_vline(xintercept = c(5.5, 28.5)) +
annotate(&amp;quot;text&amp;quot;, x = c(2.25, 16, 42),
           y = 18000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;))
Median_Week&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/median_week_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weekly-cumulative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Weekly Cumulative&lt;/h3&gt;
&lt;p&gt;This is actually one of the easier visualizations as we can make use of our previously calculated &lt;em&gt;weeknum&lt;/em&gt; to calculate the weekly cumulative sum.&lt;/p&gt;
&lt;p&gt;Why would weekly cumulative sums be interesting? Well, many interventions may focus on the weekly total steps (10,000 per day is 70,000 per week), and this plot can quickly show you when those weekly goals are met. This type of visualization also provides a way to see day-to-day variation and week-to-week variation in activity within one plot. By quickly glancing at the vertical space between linked data points one can see if there are days of either outstanding activity, or lack there of.&lt;/p&gt;
&lt;p&gt;You can also see here that the week 1 observation started on a Thursday as there are only four days of observation for week 1 (and again for week 53).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first create a new data set with an new variable that has the weekly cumulative sum of StepTotal
Daily_Steps_Week_Cumulative &amp;lt;- Daily_Steps %&amp;gt;% 
  group_by(weeknum) %&amp;gt;% 
  mutate(cs = cumsum(StepTotal))

# create plot of the weekly cumulative steps
# each line is a week using group = weeknum

Weekly_Cumulative &amp;lt;- ggplot(Daily_Steps_Week_Cumulative, aes(days, cs, group = weeknum)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,120000,20000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Weekly Cumulative Steps&amp;quot;, 
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(32.5, 193.5)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5), y = 130000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;))

Weekly_Cumulative&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/weekly_cumulative-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proportion-of-baseline-mean-and-proportion-of-baseline-median&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Proportion of Baseline Mean and Proportion of Baseline Median&lt;/h3&gt;
&lt;p&gt;In these visualizations we want to see how daily steps compared to the the mean and median of the baseline value. First we create a small data set based on our previously created &lt;code&gt;observation_data&lt;/code&gt; (see Phase Mean/Median above) that contains our baseline mean and median value, and then create our proportion data sets. After that straightforward process it’s relatively simple to generate the plots using much of the same code we’ve used previously. We also use the “end” of the baseline phase (in number of days) to create a new “days” variable so that the Intervention 1 phase starts at “day 1”.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;. I had some trouble figuring out how to properly apply a logarithmic scale to the y-axis, but a quick trip in to Stack Overflow helped me find &lt;code&gt;scale_y_log10&lt;/code&gt; and use it here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a dataframe of just data from the &amp;quot;baseline&amp;quot; phase
baseline_values &amp;lt;- observation_data %&amp;gt;% 
  filter(observation == &amp;quot;Baseline&amp;quot;)

# create a dataframe of daily proportions for &amp;quot;intervention 1&amp;quot; and &amp;quot;intervention 2&amp;quot; phases
# use the end of baseline (in days) to create new days2 variable to start &amp;quot;intervention 1&amp;quot; at day 1
proportions &amp;lt;- Daily_Steps %&amp;gt;% 
  filter(observation != &amp;quot;Baseline&amp;quot;) %&amp;gt;% 
  mutate(proportion_mean = (StepTotal / baseline_values$mean),
         proportion_median = (StepTotal / baseline_values$median),
         days2 = (days - baseline_values$end))

# create mean and medians of the proportions within each phase
# also creat start/end days for use with geom_segment to create start/stop points for horizontal lines
proportions_mean_median &amp;lt;- proportions %&amp;gt;% 
  group_by(observation) %&amp;gt;% 
  summarize(mean = mean(proportion_mean),
            median = median(proportion_median),
            start = min(days2), 
            end = max(days2))

# separate &amp;quot;intervention 1&amp;quot;  data into a dataframe for use in geom_segment
proportion_intervention1_data&amp;lt;- proportions_mean_median %&amp;gt;% 
  filter(observation == &amp;quot;Intervention 1&amp;quot;)

# separate &amp;quot;intervention 2&amp;quot;  data into a dataframe for use in geom_segment
proportion_intervention2_data&amp;lt;- proportions_mean_median %&amp;gt;% 
  filter(observation == &amp;quot;Intervention 2&amp;quot;)

# create mean proportion plot
# use scale_y_log10 and annotation_logticks to try and match original paper visualization
Proportion_Mean_Plot &amp;lt;- ggplot(proportions, aes(days2, proportion_mean, group = observation)) +
  geom_line() +
  geom_point(aes(shape = observation)) + 
  scale_shape_manual(values=c(16,0)) +
  scale_x_continuous(breaks = seq(0,335,20), expand = c(.01, .01)) +
  scale_y_log10(breaks = c(0.1, 1, 10), limits = c(0.1, 10)) +
  annotation_logticks(sides = &amp;quot;l&amp;quot;) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) + 
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Proportion of Baseline Mean&amp;quot;,
       subtitle = &amp;quot;Daily steps within each intervention phase&amp;quot;,
       x = &amp;quot;DAYS WITHIN PHASE&amp;quot;, 
       y = &amp;quot;PROPORTION OF BASELINE \n(MEAN)&amp;quot;) +
  geom_vline(xintercept = 164.5) +
  geom_hline(yintercept = 1) +
  annotate(&amp;quot;text&amp;quot;, x = c(82, 246), y = 10,
           label = c(&amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = 0, y = mean, xend = end, yend = mean),
               data = proportion_intervention1_data) +
  geom_segment(aes(x = start, y = mean, xend = end, yend = mean),
               data = proportion_intervention2_data, linetype = 3) 

Proportion_Mean_Plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/proportion_mean_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create median proportion plot
# use scale_y_log10 and annotation_logticks to try and match original paper visualization
Proportion_Median_Plot &amp;lt;- ggplot(proportions, aes(days2, proportion_median, group = observation)) +
  geom_point(aes(shape = observation)) + 
  scale_shape_manual(values=c(16,0)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0,335,20), expand = c(.01, .01)) +
  scale_y_log10(breaks = c(0.1, 1, 10), limits = c(0.1, 10)) +
  annotation_logticks(sides = &amp;quot;l&amp;quot;) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  labs(title = &amp;quot;Proportion of Baseline Median&amp;quot;,
       subtitle = &amp;quot;Daily steps within each intervention phase&amp;quot;,
       x = &amp;quot;DAYS WITHIN PHASE&amp;quot;, 
       y = &amp;quot;PROPORTION OF BASELINE \n(MEDIAN)&amp;quot;) +
  geom_vline(xintercept = 164.5) +
  geom_hline(yintercept = 1) +
  annotate(&amp;quot;text&amp;quot;, x = c(82, 246), y = 10,
           label = c(&amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = 0, y = median, xend = end, yend = median),
               data = proportion_intervention1_data, linetype = 2) +
  geom_segment(aes(x = start, y = median, xend = end, yend = median),
               data = proportion_intervention2_data, linetype = 3)

Proportion_Median_Plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/proportion_median_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;day-moving-average&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;6. 7-Day Moving Average&lt;/h3&gt;
&lt;p&gt;The 7-day moving average is one of my favorite methods for visualizing trends in messy highly variable time series data. As the authors of the paper noted, the running average method reduces variability, but it makes spotting those important trends much easier.&lt;/p&gt;
&lt;p&gt;To calculate the 7-day average we can use the &lt;code&gt;zoo&lt;/code&gt; package and it’s &lt;code&gt;rollmean&lt;/code&gt; function. Note that doing so automatically will remove the first 6 days of each phase as the seventh day in each phase is the first to have a true 7-day mean value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install zoo package if not already installed
# install.packages(&amp;quot;zoo&amp;quot;)

# load zoo package
library(zoo)

# create 7-day average data set
# we only want rolling 7-day within each observation so we use group_by first
Daily_Steps_7DMAV &amp;lt;- Daily_Steps %&amp;gt;% 
  group_by(observation) %&amp;gt;% 
  mutate(sevenmav = rollmean(StepTotal, 7, fill = NA, align = &amp;quot;right&amp;quot;))

# plot the 7-day moving average
movingavg_plot &amp;lt;- ggplot(Daily_Steps_7DMAV, aes(days, sevenmav)) +
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000), limits = c(0,35000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;7-Day Moving Average&amp;quot;, 
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS (MOVING AVERAGE)&amp;quot;) +
  geom_vline(xintercept = c(30.5, 195.5)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5), y = 38000, label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;))

movingavg_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/movingavg-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;7. Confidence Intervals&lt;/h3&gt;
&lt;p&gt;This is where my statistical knowledge begins to falter, but by relying on the methodology in the paper I was able to recreate their process for creating confidence intervals.&lt;/p&gt;
&lt;p&gt;I used the freely available SMA application from &lt;a href=&#34;http://clinicalresearcher.org/software.htm&#34;&gt;clinicalresearcher.org&lt;/a&gt; (as reference in the paper) to run the bootstrapping procedure to produce the confidence intervals. It’s important to note that the authors decided to reduce the number of data points to only the last 28 days of each phase (84 total days) in their analysis as the SMA software states that their bootstrapping method is primarily for “analyzing short streams (n&amp;lt;30 per phase) of auto-correlated time-series data.” I replicated that procedure here as well. The resulting upper and lower bounds for the 95% confidence interval were then plotted. As you can see here in the plot, there is a significant difference between the &lt;em&gt;Baseline&lt;/em&gt; phase and &lt;em&gt;Intervention 1&lt;/em&gt; and the &lt;em&gt;Baseline&lt;/em&gt; phase and &lt;em&gt;Intervention 2&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create new data sets that only include the last 28 days of each phase.
Baseline_28 &amp;lt;-  Daily_Steps %&amp;gt;% 
  filter(observation == &amp;quot;Baseline&amp;quot;, 
         days &amp;gt; 2)
Intervention1_28 &amp;lt;- Daily_Steps %&amp;gt;% 
  filter(observation == &amp;quot;Intervention 1&amp;quot;,
         days &amp;gt; (194-28))
Intervention2_28 &amp;lt;- Daily_Steps %&amp;gt;% 
  filter(observation == &amp;quot;Intervention 2&amp;quot;,
         days &amp;gt; (365-28))

# bind all those 28-day data sets together and export it as a CSV to use in SMA application. 
SMA_Data &amp;lt;- bind_rows(Baseline_28, Intervention1_28, Intervention2_28)
write_csv(SMA_Data, &amp;quot;~/Downloads/SMA_Data.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After copying the “last 28” days data into the SMA application and setting the number of iterations to 20,000, the resulting output is saved as a .txt file. You can access that &lt;a href=&#34;https://github.com/erramirez/datascience/blob/master/sevenways/SMA_Data_Output.txt&#34;&gt;output here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can then take those 95% CI values from the output for each phase and add them to the observation_date data set we created for the phase means/medians plots. Then we append those values to the full Daily_Steps data set and plot it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add 95% CI values as lowe/upper CI variables
observation_data$lowerCI &amp;lt;- c(7211.57, 11247.89, 12070.50)
observation_data$upperCI &amp;lt;- c(9480.32, 14303.11, 15935.50)

# append the CI values to the full Daily_Steps data set
Daily_Steps_CI &amp;lt;- Daily_Steps %&amp;gt;% left_join(., observation_data)

# plot the 95% CI using combination of geom_segment (for lower/upper CI lines) and geom_ribbon (for shadding)
CI_plot &amp;lt;- ggplot(Daily_Steps_CI, aes(days, StepTotal)) +
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Steps&amp;quot;,
       subtitle = &amp;quot;With 95% confidence interval&amp;quot;,
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(30, 195)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5),
           y = 38000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = start, y = lowerCI, xend = end, yend = lowerCI), data = observation_data, linetype = 2) +
  geom_segment(aes(x = start, y = upperCI, xend = end, yend = upperCI), data = observation_data, linetype = 2) +
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), alpha = .2)

CI_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/CI_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;change-point-detection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;8. Change-point Detection&lt;/h3&gt;
&lt;p&gt;Change-point detection is a very interesting method for determining when significant changes occur in time series data. Here we use the &lt;code&gt;changepoint&lt;/code&gt; package and the &lt;code&gt;cpt.mean&lt;/code&gt; function to detect the significant changes in the mean. The authors use &lt;em&gt;binnary segmentation&lt;/em&gt; and limit the number of detectable change points to be equal to the number of phase changes (2), and I apply the same methods here. We then plot those changes with horizontal dashed lines that represent the mean values for each detected significant state. I also include the default changepoint plot here for comparison.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, we don’t see any agreement with the observed changepoints as determined by the analysis and the phase changes. This is due to the arbitrary definition of our phases here, but that is not to say this changepoint method is without merit. With a “real” observed baseline/intervention one could use this simple analytical technique and quick visualization to see if changes in intervention(s) actually makes a significant difference in the outcome.&lt;/p&gt;
&lt;p&gt;I’ll also noted, that one of the more interesting uses of changepoint detection is to run this analysis “live” on data as it is created, thus being able to have a more automated way to understand significant changes in outcomes as you’re measuring them. Something that may be useful for just-in-time interventions or more personalized measurement studies. I found &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2014/breakout-detection-in-the-wild.html&#34;&gt;this blog post from the Twitter engineering group&lt;/a&gt; to be very helpful when I was wrapping my head around this methodology.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;changepoint&amp;quot;)
library(changepoint)

# create a changepoint opject using the cpt.mean function with the parameters used in the paper
change &amp;lt;- cpt.mean(Daily_Steps$StepTotal, method=&amp;quot;BinSeg&amp;quot;, Q=2)
plot(change) # the default changepoint plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/changepoint-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;change&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Class &amp;#39;cpt&amp;#39; : Changepoint Object
##        ~~   : S4 class containing 14 slots with names
##               cpts.full pen.value.full data.set cpttype method test.stat pen.type pen.value minseglen cpts ncpts.max param.est date version 
## 
## Created on  : Fri Apr 21 18:46:22 2017 
## 
## summary(.)  :
## ----------
## Created Using changepoint version 2.2.2 
## Changepoint type      : Change in mean 
## Method of analysis    : BinSeg 
## Test Statistic  : Normal 
## Type of penalty       : MBIC with value, 17.69969 
## Minimum Segment Length : 1 
## Maximum no. of cpts   : 2 
## Changepoint Locations : 94 331 
## Range of segmentations:
##      [,1] [,2]
## [1,]   94   NA
## [2,]   94  331
## 
##  For penalty values: 509315501 202651677&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add a changepoint variable based on the output of the changepoint analysis
Daily_Steps &amp;lt;- Daily_Steps %&amp;gt;% 
  mutate(changepoints = case_when(days &amp;lt;= 94 ~ 1,
                                  days &amp;lt;= 331 ~ 2,
                                  days &amp;gt; 331 ~ 3))

# create a data set of mean values for each change point to be used with geom_segment in the plot
change_means &amp;lt;- Daily_Steps %&amp;gt;% 
  group_by(changepoints) %&amp;gt;% 
  summarize(mean = mean(StepTotal),
         start = min(days),
         end = max(days))

# plot it!
changepoint_plot &amp;lt;- ggplot(Daily_Steps, aes(days, StepTotal)) +
  geom_line(size = .35) +
  geom_point() +
  scale_x_continuous(breaks = seq(0,365,20), expand = c(.01, .01)) +
  scale_y_continuous(breaks = seq(0,35000,5000), limits = c(0,35000)) +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;)) +
  theme(axis.line.x = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1),
        axis.line.y = element_line(colour = &amp;quot;black&amp;quot;, size = 0.5, linetype = 1)) +
  theme(axis.text.x = element_text(size = 6)) +
  labs(title = &amp;quot;Daily Steps&amp;quot;,
       subtitle = &amp;quot;With detected change points&amp;quot;,
       x = &amp;quot;DAYS&amp;quot;, 
       y = &amp;quot;STEPS&amp;quot;) +
  geom_vline(xintercept = c(30, 195)) +
  annotate(&amp;quot;text&amp;quot;, x = c(15, 112.5, 277.5),
           y = 38000, 
           label = c(&amp;quot;BL&amp;quot;, &amp;quot;Intervention 1&amp;quot;, &amp;quot;Intervention 2&amp;quot;)) +
  geom_segment(aes(x = start, y = mean, xend = end, yend = mean), data = change_means, linetype = 2) 

changepoint_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-09-10-examining-variability-in-phsyical-activity-with-r_files/figure-html/changepoint-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;Well, that was fun! I know I learned a lot more about using ggplot, and some analytic methods, that can be used for examining time series data. I’m lucky to have &lt;em&gt;a lot&lt;/em&gt; of time series data at my disposal to play around with these visualization methods and I hope this can be useful for you as well.&lt;/p&gt;
&lt;p&gt;More broadly, I hope this this just another gentle push for our research community to share more code and examples alongside their published work. Sharing is caring folks!&lt;/p&gt;
&lt;p&gt;As always, comments and edits are welcome. Feel free to submit a pull request or issue on Github, or just &lt;a href=&#34;http://www.twitter.com/eramriez&#34;&gt;ping me on twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How Much Fun is Maximum Fun?</title>
      <link>/post/2016/06/04/how-much-fun-is-maximum-fun/</link>
      <pubDate>Sat, 04 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/06/04/how-much-fun-is-maximum-fun/</guid>
      <description>&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;I’m a big consumer of podcasts. Ever since I started living on my own while in graduate school I’ve found that having funny and interesting people in my ears helps me get through the day. Even now that I’m cohabiting with my wife I haven’t left my trusty podcasts behind. They’re great for the long commutes back and forth to San Diego, for reducing my stress while stuck in LA traffic, and for making my laugh while I cook, clean, and exercise.&lt;/p&gt;
&lt;p&gt;I’m a fan and donor (support the things you love!) of one podcast network in particular, &lt;a href=&#34;http://www.maximumfun.org&#34;&gt;Maximum Fun&lt;/a&gt;, so much so that I’m a semi-regular participant in their Facebook group and subreddit. Recently, someone in the Facebook group asked for some data about the network, in particular the number of shows that have been published, in order to visualize the growth of the network. As someone who’s keen to keep my data analysis skills fresh and nimble I took this as an opportunity to dive back into R. Here’s what I’ve done so far:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gathering-data-from-an-rss-feed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gathering Data from an RSS Feed&lt;/h2&gt;
&lt;p&gt;Podcasts are unique in that they’re basically just a simple feed of audio files. That feed has data embedded into it that we can access and save. I’m pretty new to web-scrapping, but I was able to find a really nice example of &lt;a href=&#34;https://gist.github.com/izahn/5785265&#34;&gt;how to scrape an RSS feed in R here&lt;/a&gt;. I adapted that to scrape and save data from each of the podcasts in the Maximum Fun network. Here’s an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RCurl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: bitops&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(XML)

options(stringsAsFactors = FALSE)

## get rss document
xml.url &amp;lt;- &amp;quot;http://adventurezone.libsyn.com/rss&amp;quot;
script &amp;lt;- getURL(xml.url, ssl.verifypeer = FALSE)

## convert document to XML tree in R
doc &amp;lt;- xmlParse(script)
## find the names of the item nodes

## Extract some information from each node in the rss feed
titles &amp;lt;- xpathSApply(doc,&amp;#39;//item/title&amp;#39;,xmlValue)
date &amp;lt;- xpathSApply(doc,&amp;#39;//item/pubDate&amp;#39;,xmlValue)
duration &amp;lt;- xpathSApply(doc,&amp;#39;//item/itunes:duration&amp;#39;,xmlValue)

# create data frame with important variables
Adventurezone &amp;lt;- data.frame(titles, date, duration)

# create unique identifier
Adventurezone$id &amp;lt;- &amp;quot;The Adventure Zone&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I probably could have created a function to run through all the shows, but instead I used that chunk for every show. It was actually useful as a few of the shows had missing episodes or titles and durations that didn’t match up.&lt;/p&gt;
&lt;p&gt;Once I had all the data scrapped from the feeds I was able to combine it into one dataset of 4,202 episodes from 25 different shows. The date/duration variables were pretty messy so I noodled around a bit and cleaned them up into something manageable. I’ve saved that final data in &lt;a href=&#34;https://dl.dropboxusercontent.com/u/2513399/MaximumFun.rdata&#34;&gt;Rdata&lt;/a&gt; and &lt;a href=&#34;https://dl.dropboxusercontent.com/u/2513399/MaximumFun.csv&#34;&gt;.csv&lt;/a&gt; formats if you want to play with them yourself. I’m loading the RData file here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(url(&amp;quot;https://www.dropbox.com/s/5l94kzc54s177uh/MaximumFun.rdata?dl=1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;p&gt;Once we have all the data in a good format creating visualizations is actually pretty easy! Let’s start with a simple bar chart that plots the number of shows per month:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scales)
# simple bar chart with sum of number of shows per month
mf.stacked.bar &amp;lt;- ggplot(MaximumFun, aes(monthyear)) + geom_bar() +
  labs(title = &amp;quot;Number of Shows on Maximum Fun per Month&amp;quot;, x = &amp;quot;Year - Month&amp;quot;, y = &amp;quot;Number of Shows Published&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))
mf.stacked.bar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/barchart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s not bad, but what if we wanted to know which shows were on the network over time? We can use the “id” variable we created in the initial data scrapping process to label each show. This visualization needs a better color palette to better differentiate between each show, but I’ll leave it here for now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scales)
# add in color to represent the shows
# needs a better color palette
mf.stacked.bar2 &amp;lt;- ggplot(MaximumFun, aes(monthyear, fill = id)) + geom_bar() +
  labs(title = &amp;quot;Number of Shows on Maximum Fun per Month&amp;quot;, x = &amp;quot;Year - Month&amp;quot;, y = &amp;quot;Number of Shows Published&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6), legend.position=&amp;quot;bottom&amp;quot;)
mf.stacked.bar2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/barchart_shows-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What can we find out about each show? Let’s start with visualizing the total number of hours each podcast has published:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scales)
mf.stacked.bar4 &amp;lt;- ggplot(MaximumFun, aes(id, (showlength/60))) + geom_bar(stat=&amp;quot;identity&amp;quot;) +
  labs(title = &amp;quot;Total Duration of Each Show on Maximum Fun&amp;quot;, x = &amp;quot;Show&amp;quot;, y = &amp;quot;Total Number of Hours&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
mf.stacked.bar4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 32 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/show_hours-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How about the number of episodes per show?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simple bar chart for total number of episodes per show
mf.stacked.bar5 &amp;lt;- ggplot(MaximumFun, aes(id)) + geom_bar() +
  labs(title = &amp;quot;Number of Episode of each Show on Maximum Fun&amp;quot;, x = &amp;quot;Show&amp;quot;, y = &amp;quot;Total Number of Episodes&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
mf.stacked.bar5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/show_episodes-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I also got around to reformatting the data so that we could look at the number of shows and amount of content produced by Maximum Fun over time. To do that we first have to create a new dataset that aggregates some of the information:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
# number of shows on the network over time
MaxFunShows &amp;lt;- ddply(MaximumFun, c(&amp;quot;monthyear&amp;quot;), summarise,
                           &amp;#39;NumberofShows&amp;#39; = length(unique(id)),
                           &amp;#39;DurationofShows&amp;#39; = sum(showlength, na.rm=TRUE)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lubridate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;lubridate&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:plyr&amp;#39;:
## 
##     here&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     date&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can make plots just like the ones we have above!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mf.stacked.bar5 &amp;lt;- ggplot(MaxFunShows, aes(monthyear, NumberofShows)) + geom_bar(stat=&amp;quot;identity&amp;quot;) +
  labs(title = &amp;quot;Number of Shows on Maximum Fun over time&amp;quot;, x = &amp;quot;Date&amp;quot;, y = &amp;quot;Number of Shows on the Network&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))
mf.stacked.bar5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/barchart_number_shows-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about the amount of content over time?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mf.stacked.bar5 &amp;lt;- ggplot(MaxFunShows, aes(monthyear, (DurationofShows/60))) + geom_bar(stat=&amp;quot;identity&amp;quot;) +
  labs(title = &amp;quot;Amount of Content Produced (in hours) by Maximum Fun over time&amp;quot;, x = &amp;quot;Date&amp;quot;, y = &amp;quot;Hours&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))
mf.stacked.bar5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-06-04-how-much-fun-is-maximum-fun_files/figure-html/barchart_networkhours-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, this doesn’t include some of the great shows that have moved on to be either independtly operated or part of another network, but it’s still a pretty good approximation of the growth over time.&lt;/p&gt;
&lt;p&gt;I’ll probably keep noodling around with this data. Probably a lot more I can do with visualizing particular shows and the network as a whole. If you have ideas &lt;a href=&#34;http://twitter.com/eramirez&#34;&gt;get in touch&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I&#39;m the Doctor Now</title>
      <link>/post/2016/05/27/im-the-doctor-now/</link>
      <pubDate>Fri, 27 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/05/27/im-the-doctor-now/</guid>
      <description>&lt;p&gt;Well, it&amp;rsquo;s finally done. Eight years after I started this crazy graduate school journey I&amp;rsquo;ve successfully defended my dissertation. It wasn&amp;rsquo;t easy, it wasn&amp;rsquo;t perfect, but it&amp;rsquo;s done and I couldn&amp;rsquo;t be happier to have accomplished this milestone.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/dissertation.jpeg&#34; alt=&#34;dissertation&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Some folks have asked to see the slides, so I thought I&amp;rsquo;d share them here. Hopefully they make sense, but don&amp;rsquo;t worry if they don&amp;rsquo;t. I&amp;rsquo;ll be posting more about this work in the future.&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;350de98f9213495c886f75311af89fd3&#34; data-ratio=&#34;1.33333333333333&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Time Series Data</title>
      <link>/post/2016/01/25/visualizing-time-series-data/</link>
      <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/01/25/visualizing-time-series-data/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When working with human behavior you’ll almost always have to deal with data that is formatted as a time series. Briefly, time series data corresponds with repeated measures of a variable (or multiple variables) at consistent intervals over a period.&lt;/p&gt;
&lt;p&gt;At the Center for Wireless and Population Health Systems, we’re consistently dealing with physical activity data collected from accelerometers worn by study participants. These measurement devices sample human locomotion at various rates over periods lasting days, weeks, or even longer. There are a variety of methods to process and analyze accelerometer data, but we know any good statistician will use a variety of visualization techniques so become familiar with the data and understand it better. This document is meant to be an introduction to different methods for visualizing time series data. By no means does it cover &lt;em&gt;every&lt;/em&gt; method, but it should get you started and give you some ideas for additional techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;The data used in this example was collected as part of my doctoral dissertation research. The physical activity data was gathered from participants who used a Fitbit activity tracker to measure their physical activity. Access to historical data was granted by participants and downloaded using &lt;a href=&#34;http://fitabase.com&#34;&gt;Fitabase&lt;/a&gt;. A variety of data was made available, but for this example we’ll be focusing on steps.&lt;/p&gt;
&lt;p&gt;As my dissertation analysis is ongoing, I’m using my own Fitbit data here for this example.&lt;/p&gt;
&lt;div id=&#34;step-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step Data&lt;/h2&gt;
&lt;p&gt;I downloaded my step data over a two-year period from Jan. 1, 2012 to Jan. 1, 2014. Two data files will be used in this analysis (click to download the data sets):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/n4a398int2ffig9/ER_FitbitDailySteps_2012.csv?d1=1&#34;&gt;Daily Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/csmddz7o6sfny52/ER_FitbitMinuteSteps_2012.csv?dl=1&#34;&gt;Minute-Level Steps&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;Daily Steps&lt;/strong&gt; file contains 366 observations of the total amount of steps recorded for each day in 2012 (2012 was a leap year). The &lt;strong&gt;Minute-Level Steps&lt;/strong&gt; file contains 527,040 observations (1,440 minutes per hour, 24 hours, 366 days) of the total steps recorded per minute for 2012.&lt;/p&gt;
&lt;p&gt;Let’s load ’em up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DailySteps &amp;lt;- read.csv(&amp;quot;https://www.dropbox.com/s/n4a398int2ffig9/ER_FitbitDailySteps_2012.csv?dl=1&amp;quot;)
MinuteSteps &amp;lt;- read.csv(&amp;quot;https://www.dropbox.com/s/csmddz7o6sfny52/ER_FitbitMinuteSteps_2012.csv?dl=1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cleaning the data&lt;/h1&gt;
&lt;p&gt;So the data is loaded and a quick glance indicates that the date and date/time vectors were imported as &lt;em&gt;factors&lt;/em&gt;. That’s all well and good for most things, but we’re going to want dates as dates and times as times for visualization purposes. Let’s get those corrected. I like using the &lt;a href=&#34;https://cran.r-project.org/web/packages/lubridate/index.html&#34;&gt;lubridate package&lt;/a&gt; as it handles dates and times very easily. The &lt;em&gt;ActivityDay&lt;/em&gt; vector is in mm/dd/yyyy format so we can use the &lt;em&gt;mdy&lt;/em&gt; function from lubridate to change it into a POSIXct variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
DailySteps$Date &amp;lt;- mdy(DailySteps$ActivityDay)
DailySteps$Date &amp;lt;- as.Date(DailySteps$Date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minute-level data is a bit trickier. The &lt;em&gt;ActivityMinute&lt;/em&gt; vector is formatted for date/time in mm/dd/yyyy hh:mm:ss AM/PM. This isn’t super useful for instance, when we want to plot the minute-by-minute steps for a full day of data. So let’s do some reformatting to clean it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinuteSteps$ActivityMinute &amp;lt;- mdy_hms(as.character(MinuteSteps$ActivityMinute)) 
MinuteSteps$Date &amp;lt;- as.Date(MinuteSteps$ActivityMinute, format = &amp;quot;%Y-%m-%d  %H:%M:%S&amp;quot;)
MinuteSteps$Time &amp;lt;- format(as.POSIXct(strptime(MinuteSteps$ActivityMinute, &amp;quot;%Y-%m-%d  %H:%M:%S&amp;quot;,tz=&amp;quot;&amp;quot;)) ,format = &amp;quot;%H:%M&amp;quot;)
MinuteSteps$Time &amp;lt;- as.POSIXct(MinuteSteps$Time, format = &amp;quot;%H:%M&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why four steps to create two variables? Well, we want to make the date, a date variable, and we also want to strip the time to it’s own variable. strptime can do that, but it returns a character vector, which ggplot won’t like when use it as a scale. We have to convert that time character vector back into POSIXct and by doing so we assign it an arbritrary date. If you don’t asssign it in the function then it will automatically assign today’s date.&lt;/p&gt;
&lt;div id=&#34;bar-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bar Plots&lt;/h2&gt;
&lt;p&gt;I find bar plots a bit easier to read when dealing with aggregated data such as daily step counts. This might be because I envision the volume of the bar to contain the total amount of steps for that day. So let’s make a quick bar plot of the Daily Steps.&lt;/p&gt;
&lt;p&gt;Before we begin we’ll need to load two packages: &lt;strong&gt;ggplot2&lt;/strong&gt; and &lt;strong&gt;scales&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scales)
DailyStepsPlot &amp;lt;- ggplot(DailySteps, aes(x=Date, y=StepTotal)) #This will base plot which we&amp;#39;ll manipulate.

DailyStepsPlot + geom_bar(stat=&amp;quot;identity&amp;quot;) + scale_x_date(breaks=date_breaks(&amp;quot;1 month&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/bar_plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Line Plots&lt;/h2&gt;
&lt;p&gt;What about a line plot (with added points)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DailyStepsPlot + geom_line(colour=&amp;quot;blue&amp;quot;) + geom_point(colour=&amp;quot;blue4&amp;quot;) + scale_x_date(breaks=date_breaks(&amp;quot;3 month&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/linepoint_plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Line Plots&lt;/h2&gt;
&lt;p&gt;Line plots aren’t that great when you have a continuous data set that spans over 500,000 observations. For a data set that large we’re typically trying to look into patterns. A good way to do that is to plot the data one level up, such as per day in this case.&lt;/p&gt;
&lt;p&gt;We’ll plot each day on the same canvas with each line set to a transparency of alpha = .05. I also cut down the line size so that patterns might be seen in the resulting banding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(MinuteSteps, aes(x=Time, y=Steps, group=Date)) + 
  geom_path(size=.5, alpha = 0.05, colour=&amp;quot;blue&amp;quot;) + 
  scale_x_datetime(breaks=date_breaks(&amp;quot;2 hour&amp;quot;), labels=date_format(&amp;quot;%H:%M&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/multiple_lines-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see some clear banding at 120 steps/min or so and another a bit lower around 75 steps/min. It also appears that there a few days with periods of high activity (&amp;gt;150 steps/min) just past 6PM (18:00). But, could it be clearer?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scatterplot&lt;/h2&gt;
&lt;p&gt;When using the line graph, multiple lines can obscure the patterns we’re trying to tease out by introducing a bit of visual noise. Let’s reduce the noise by taking away the lines and using just points in a scatterplot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(MinuteSteps, aes(x=Time, y=Steps, group=Date)) + 
  geom_point(size=.5, alpha = 0.1, colour=&amp;quot;blue&amp;quot;) + 
  scale_x_datetime(breaks=date_breaks(&amp;quot;2 hour&amp;quot;), labels=date_format(&amp;quot;%H:%M&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/scatterplot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-daily-data-multiple-lines-facets-wrapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grouping Daily Data (Multiple Lines &amp;amp; Facets Wrapping)&lt;/h2&gt;
&lt;p&gt;So we’ve plotted every day, and we’ve plotted every minute in our data. We’ve teased out a few simple patterns, but what about other distinctions in the data?&lt;/p&gt;
&lt;p&gt;One of the methods common in physical activity data analysis is to explore differences across different days of the week. Are individuals more active on certain days of the week? How about the difference between weekdays and weeekend days?&lt;/p&gt;
&lt;p&gt;To be able to create visualizations that help us understand these potential differences we’ll have to do a bit of work on the data.&lt;/p&gt;
&lt;p&gt;First, let’s create a &lt;em&gt;Day&lt;/em&gt; variable for our Daily data set. Again, we’re turning to the &lt;strong&gt;lubridate&lt;/strong&gt; package. Lubridate has function called &lt;strong&gt;&lt;em&gt;wday&lt;/em&gt;&lt;/strong&gt; which will return the day of the week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DailySteps$Day &amp;lt;- wday(DailySteps$Date, label = TRUE) # we use labels=true here to return day labels instead of numeric values. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see if we can plot differences between days:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(DailySteps, aes(x=Date, y=StepTotal, group=Day, colour=Day)) + geom_path() + geom_point() + scale_x_date(breaks=date_breaks(&amp;quot;1 month&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/alldays_lineplot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, that’s a mess. What if we graph each day individually? We can do this by calling creating multiple facets of the same plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(DailySteps, aes(x=Date, y=StepTotal, colour=Day)) + 
  geom_path() + 
  geom_point() + 
  scale_x_date(breaks=date_breaks(&amp;quot;1 month&amp;quot;)) + 
  facet_grid(Day ~.) +
  theme(legend.position=&amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/alldays_facet-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a bit better, but I personally think the week starts on Monday, not Sunday. Let’s fix that in our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DailySteps$Day2 &amp;lt;- factor(DailySteps$Day, levels = c(&amp;quot;Mon&amp;quot;, &amp;quot;Tues&amp;quot;, &amp;quot;Wed&amp;quot;, &amp;quot;Thurs&amp;quot;, &amp;quot;Fri&amp;quot;, &amp;quot;Sat&amp;quot;, &amp;quot;Sun&amp;quot;))

ggplot(DailySteps, aes(x=Date, y=StepTotal, colour=Day2)) + 
  geom_path() + 
  geom_point() + 
  scale_x_date(breaks=date_breaks(&amp;quot;1 month&amp;quot;)) + 
  facet_grid(Day2 ~.) +
  theme(legend.position=&amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/alldays_facet_M-S-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-minute-data-multiple-lines-facets-wrapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grouping Minute Data (Multiple Lines &amp;amp; Facets Wrapping)&lt;/h2&gt;
&lt;p&gt;Okay, that was fun. But let’s get to the minute data. We’re basically going to do the same type of processing on the minute level data to get day of the week and then we’ll create a few different plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinuteSteps$Day &amp;lt;- wday(MinuteSteps$Date, label = TRUE) 
MinuteSteps$Day2 &amp;lt;- factor(DailySteps$Day, levels = c(&amp;quot;Mon&amp;quot;, &amp;quot;Tues&amp;quot;, &amp;quot;Wed&amp;quot;, &amp;quot;Thurs&amp;quot;, &amp;quot;Fri&amp;quot;, &amp;quot;Sat&amp;quot;, &amp;quot;Sun&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you might be able to guess, plotting all the minute-level data points, even when coloured by group, is a complete mess. We’ll start with a facetted plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(MinuteSteps, aes(x=Time, y=Steps, colour=Day2)) + 
  geom_point(size=.5, alpha = 0.1) + 
  scale_x_datetime(breaks=date_breaks(&amp;quot;2 hour&amp;quot;), labels=date_format(&amp;quot;%H:%M&amp;quot;)) +
  facet_grid(Day2 ~.) +
  theme(legend.position=&amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/minute_point_facet-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, that’s nice to look at, but still hard to understand any type of trends or patterns. Since we have so much data, we can probably better understand what’s going on by collapsing it. For instance, we might want to know what an “average Monday” might look like. That’s easy to do!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinuteSteps.AvgDays &amp;lt;- aggregate(Steps ~ Time + Day2, MinuteSteps, mean)

ggplot(MinuteSteps.AvgDays, aes(x=Time, y=Steps, colour=Day2)) + 
  geom_path() + 
  scale_x_datetime(breaks=date_breaks(&amp;quot;2 hour&amp;quot;), labels=date_format(&amp;quot;%H:%M&amp;quot;)) +
  facet_grid(Day2 ~.) +
  theme(legend.position=&amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-02-23-visualizing-time-series-data_files/figure-html/minute_path_facet-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ode to the Outshot</title>
      <link>/post/2014/03/19/ode-to-the-outshot/</link>
      <pubDate>Wed, 19 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014/03/19/ode-to-the-outshot/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;I think the &lt;a href=&#34;https://twitter.com/Bullseye&#34;&gt;@Bullseye&lt;/a&gt; out shots are my new favorite piece of radio.&lt;/p&gt;&amp;mdash; Ernesto Ramirez (@eramirez) &lt;a href=&#34;https://twitter.com/eramirez/statuses/408084903238987776&#34;&gt;December 4, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I&amp;rsquo;ve been listening to podcasts for a while. Like most people I started with the juggernaut, This American Life, but quickly opened up my catalogue to other shows and ideas. I&amp;rsquo;m sure if you  were to plot my podcast listening statistics you&amp;rsquo;d see a sharp uptick in hours listened when I started my second round of graduate school almost six years ago in San Diego.&lt;/p&gt;

&lt;p&gt;I moved to San Diego with the belief that my girlfriend (now wife) wouldn&amp;rsquo;t be far behind. I&amp;rsquo;d find a place to settle, get started with the hard business of academic scholarship, and then she&amp;rsquo;d come join me. Six months was our guess. Funny thing about guesses, they&amp;rsquo;re not very reliable.&lt;/p&gt;

&lt;p&gt;Fast forward four months and I was spending Thanksgiving day with my girlfriend eating Boston Market takeout while we took a break form unpacking her belongings at her new apartment in Bristol, CT. She&amp;rsquo;d landed an entry into her dream job. A job that placed her over 2,800 miles away from where I was pursuing my dreams. Not fun, but we preserved.&lt;/p&gt;

&lt;p&gt;All of this is to say that I was alone a lot of the time for the  four years before we synced back up in the same time zone, found our first apartment together, and got married. Sure I had friends, and work, and classes, but there was also the solo meals I made in a old studio apartment or the long walks I took in the barrio (I moved around a lot). During those moments of solitude my companion was always my iPhone and my podcasts.&lt;/p&gt;

&lt;p&gt;Like I said, I&amp;rsquo;ve listened to a bunch. Other&amp;rsquo;s have listened to more, some less. I&amp;rsquo;m probably right in the middle. I stick to comedy and culture mostly. &lt;a href=&#34;http://www.nerdist.com/podcast/nerdist/&#34;&gt;Chris Hardwick&lt;/a&gt; and his knuckled head friends, Matt and Jonah, kept me company on a lot of walks, runs, and bus commutes. &lt;a href=&#34;http://www.radiolab.org/&#34;&gt;Jad and Robert&lt;/a&gt; were a go to for long plane rides. &lt;a href=&#34;http://www.wtfpod.com/&#34;&gt;Marc&lt;/a&gt; came and went depending on my mood. &lt;a href=&#34;http://douglovesmovies.com/&#34;&gt;Doug&lt;/a&gt; was (and is) an entertaining diversion. And of course I can&amp;rsquo;t forget the two pillars &lt;a href=&#34;http://99percentinvisible.org/&#34;&gt;Roman&lt;/a&gt; and &lt;a href=&#34;http://www.thisamericanlife.org/&#34;&gt;Ira&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When I was listening to these voices the same name was mentioned  time and time again - Jesse Thorn. I also kept hearing something about The Sound of Young America off and on, but never really gave it much thought. Then I started listening to John Hodgman dispense Internet Justice on the &lt;a href=&#34;http://maximumfun.org/shows/judge-john-hodgman&#34;&gt;Judge John Hodgman podcast&lt;/a&gt;. Why? I don&amp;rsquo;t really remember. Probably a recommendation online. Maybe Roman? Who knows. It doesn&amp;rsquo;t matter. John was great (still is) and his sidekick, was&amp;hellip; you guessed it, Mr. Jesse Thorn.&lt;/p&gt;

&lt;p&gt;It took a while, probably about six months of listening to Jesse make funny side comments and endearing commentary for me to finally take the time to check out what this whole MaximumFun thing was all about. I dipped my toes in the water with &lt;a href=&#34;http://maximumfun.org/shows/jordan-jesse-go&#34;&gt;Jordan, Jesse, Go!&lt;/a&gt;. It was  great. Funny, smart, irreverent, but sincere and full of truth. I loved it right away. I started listening to the back catalogue. For some reason I went about it the wrong way and just kept going back one episode at a time instead of starting from the beginning. It was a stupid way to do it, but it didn&amp;rsquo;t matter, each joke was still funny. Each story about getting married, having children, moving, getting jobs - basically becoming an adult - was real and moving and often hilarious. Somewhere along the way in my JJGO historical listening project I decided to branch out again and listen to what used to be the Sound of Young America and is now &lt;a href=&#34;http://www.maximumfun.org/shows/bullseye&#34;&gt;Bullseye&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bullseye was scary and odd to me. It wasn&amp;rsquo;t what I was used to. It was serious. It was interview-based with different segments, but it didn&amp;rsquo;t have an overarching story like a This American Life episode. Plus, I&amp;rsquo;ll be honest, some of the guests just weren&amp;rsquo;t all that interesting to me. But I gave it a try. And I&amp;rsquo;m better for it.&lt;/p&gt;

&lt;p&gt;Jesse likes to say, &amp;ldquo;Bullseye is at it&amp;rsquo;s core, a recommendation show.&amp;rdquo; It&amp;rsquo;s Jesse recommending bits of culture new and old, through his smart, funny, and engaging interviews with the people that make it. It&amp;rsquo;s people like Mark Fraunfelder from BoingBoing or the AV Club telling you about the music, books, and games that you simply must try because &lt;em&gt;they love them&lt;/em&gt;. It&amp;rsquo;s also people talking about the music that&amp;rsquo;s changed their lives.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Stop right here and listen to recent Academy Award winner &lt;a href=&#34;https://soundcloud.com/bullseye-with-jesse-thorn/bobby-lopez-stcml&#34;&gt;Bobby Lopez talk about&lt;/a&gt; &amp;ldquo;Pure Imagination&amp;rdquo; from Charlie and the Chocolate Factory. Really, stop. &lt;a href=&#34;https://soundcloud.com/bullseye-with-jesse-thorn/bobby-lopez-stcml&#34;&gt;Listen&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But the best part of Bullseye, and my current favorite piece of radio, is the Outshot. Each week Bullseye wraps up with a personal recommendation from Jesse  Thorn. These aren&amp;rsquo;t just reviews of the latest albums or movies. It&amp;rsquo;s something deeper and more intense than almost anything you&amp;rsquo;ll hear out there today. Jesse opens himself up the audience to talk about something he really enjoys. It&amp;rsquo;s sincere and heartfelt, &lt;a href=&#34;http://www.nytimes.com/2008/09/07/fashion/weddings/07VOWS.html?_r=1&amp;amp;scp=1&amp;amp;sq=jesse%20thorn&amp;amp;st=cse&#34;&gt;because that&amp;rsquo;s just who he is&lt;/a&gt;. In his wedding announcement in the New York Time nearly six years ago his wife mentions his inability to be insincere, &amp;ldquo;He is not capable of it,&amp;rdquo; she says. &amp;ldquo;He’s so honest and straightforward about what he likes and doesn’t like, and what he’s thinking. And that’s something I admire.” I admire it too. Not just because he&amp;rsquo;s genuine, but also because he&amp;rsquo;s unafraid to share the things he finds endearing and delightful even though others may scoff. He&amp;rsquo;ll to tell you why &lt;a href=&#34;https://soundcloud.com/bullseye-with-jesse-thorn/outshot-babe&#34;&gt;Babe: Pig in the City&lt;/a&gt; is really a heroes tale or why &lt;a href=&#34;https://soundcloud.com/bullseye-with-jesse-thorn/the-outshot-the-muppet-movie&#34;&gt;the Muppet Movie&lt;/a&gt; is about friends and artists dreaming about how to make the world a better place.&lt;/p&gt;

&lt;p&gt;I love the Outshot because Jesse loves these pieces of culture enough to share them with us. And with each segment and his signature sign off, Jesse does what most of use spend our entire lives trying to do, he draws his bow, let his arrow fly and hit the bullseye, again and again. Thanks Jesse.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Epilogue&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So I actually recored my own &amp;ldquo;Ode to the Outshot&amp;rdquo; and you can go &lt;a href=&#34;https://soundcloud.com/erramirez/ode-to-the-outshot&#34;&gt;listen to it here&lt;/a&gt;. It&amp;rsquo;s also embedded below.&lt;/p&gt;

&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/140653808&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_artwork=true&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;This week is also MaxFunDrive. The yearly pledge drive for the MaximumFun network and the shows they help bring to the world. This year I&amp;rsquo;ve pitched in and become a donor. Each week I listen to about 4-5 hours of MaxFun programming and I&amp;rsquo;m more than happy to chip in what amounts to $1 per episode. That&amp;rsquo;s cheap considering how much entertainment I&amp;rsquo;m getting. If you&amp;rsquo;re a new or old listener you should consider chipping in too. All the &lt;a href=&#34;https://twitter.com/romanmars/status/446427703907213312&#34;&gt;cool kids&lt;/a&gt; are doing it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As always, comments are welcome. This post is available &lt;a href=&#34;https://github.com/erramirez/erramirez.github.io/blob/master/_posts/2014-03-19-Ode-to-the-Outshot.md&#34;&gt;on Github&lt;/a&gt; if that’s your style, and &lt;a href=&#34;https://medium.com/ernesto-ramirez/38132ec3ac12&#34;&gt;Medium&lt;/a&gt; if you like that platform. Free free to connect with me on &lt;a href=&#34;https://medium.com/ernesto-ramirez/83680d1a8c1a&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;mailto:er.ramirez@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Hardware is Hard</title>
      <link>/post/2014/03/18/hardware-is-hard/</link>
      <pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014/03/18/hardware-is-hard/</guid>
      <description>&lt;p&gt;&lt;em&gt;I&amp;rsquo;ve been thinking a lot lately about stuff I&amp;rsquo;ve written, but is lost in other ecosystems. I got a Quora notification this evening and it reminded me of something I wrote on that site over two years ago in response to the question, &amp;ldquo;How hard is it for a startup to design hardware like Jawbone UP, Nike+ Fuelband, Fitbit, etc.?&amp;rdquo; I really like my answer, but I&amp;rsquo;m biased (and &amp;ldquo;me&amp;rdquo; is a small sample size). I&amp;rsquo;ve reproduced it here with a few edits and added updates via footnotes. Let me know what you think.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There is a lot of room to improve upon the design, UI/UX, and hardware of physical activity and tracking devices. For instance, I noticed you didn&amp;rsquo;t mention &lt;a href=&#34;mybasis.com&#34;&gt;Basis&lt;/a&gt;. They are bringing a great new device to market this year that will integrate optically sensed HR, galvanic skin response, and accelerometery in a nicely designed watch.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;When you talk about designing hardware you have to think not just about what is possible now, but also what will be possible over the next five to ten years and begin designing products that take advantage of new &lt;a href=&#34;http://en.wikipedia.org/wiki/Microelectromechanical_systems&#34;&gt;MEMs&lt;/a&gt; technology. For instance, there is a lot of great R&amp;amp;D coming out of MIT that is supporting the creation of &lt;a href=&#34;http://web.mit.edu/newsoffice/2010/accelerometer-0416.html&#34;&gt;even smaller sensors&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another design element to think of is functionality. Not in the sense of does the device work, but rather, how it works around your normal everyday life. My fiancé was considering all three you mention (FitBit, Nike+ Fuelband, UP) and decided to go with the FitBit because she didn&amp;rsquo;t want to be tied to wearing something on her wrist every day that isn&amp;rsquo;t a watch (she&amp;rsquo;s old school).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; With the new class of open-source wearable microprocessors (&lt;a href=&#34;http://arduino.cc/en/Main/arduinoBoardLilyPad&#34;&gt;LillyPad&lt;/a&gt;, &lt;a href=&#34;http://www.adafruit.com/category/92&#34;&gt;Flora&lt;/a&gt;, etc.) I don&amp;rsquo;t see why in the next few years we can&amp;rsquo;t have our trackers embedded in our clothing.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This is getting long, but I&amp;rsquo;ll bring up another hardware design that I see really separating the industry - wireless connectivity. I don&amp;rsquo;t mean bluetooth, I mean true send the data to cloud, kind of connectivity. Qualcomm is pioneering this initiative with their new &lt;a href=&#34;http://www.qualcommlife.com/&#34;&gt;Qualcomm Life&lt;/a&gt; venture  and their proprietary machine-to-machine systems. There will be a day soon, if not this year then next, where your FitBit or Nike+ Fuelband or whatever YOU make has a similar chipset to what you find in the Kindle 3G. This is a huge design challenge as battery usage will be altered dramatically, but again nothing that is insurmountable.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Lastly, I think (and I&amp;rsquo;m a bit biased as I&amp;rsquo;m a behavioral scientist)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, is that the technology community tends to overly focus on the hardware rather than the user experience design. Yes, it has to work well as Jawbone showed us. Yes, it has look good - Jawbone showed us that too. But, it also has to be tied to an engaging and worthwhile experience. The importance of good design cannot be overstated. Look at what Aza Raskin is doing over at &lt;a href=&#34;http://massivehealth.com/&#34;&gt;Massive Health&lt;/a&gt; and their Eatery app.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; Design will win every time. If you&amp;rsquo;re really thinking about building hardware make sure you find an Aza clone (good luck with that) and spend as much if not more time creating, testing, and iterating on the user experience. People will use something because it&amp;rsquo;s cool and different, but the world will use the thing that works, the thing that makes them forget they&amp;rsquo;re interacting with 1&amp;rsquo;s and 0&amp;rsquo;s (you only need to look at the iPhone for confirmation about the importance of experience design).&lt;/p&gt;

&lt;p&gt;Oh, and I guess to really answer your question. It is very, very, very hard to make physical products that work. That doesn&amp;rsquo;t mean it isn&amp;rsquo;t worth it. Fitbit is a great example. They went through many trials after their huge showing at Techcrunch50 in 2008. They missed launch dates, they had some product failures, but they persevered. &lt;a href=&#34;http://allthingsd.com/20120124/amid-increasing-competition-fitbit-scores-12-million-in-funding/&#34;&gt;Now look&lt;/a&gt; at them.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I guess in the end you might be asking the wrong question. Of course it is hard, building cars is hard, doing astrophysics is hard, playing the cello is hard. But here we are with hundreds of cars to choose from, new PhDs staring at the stars every night, and parents enrolling their children in music lessons. Hard will never go away, but being better is always within reach.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As always, comments are welcome. This post is available &lt;a href=&#34;https://github.com/erramirez/erramirez.github.io/blob/master/_posts/2014-03-19-Hardware-Is-Hard.md&#34;&gt;on Github&lt;/a&gt; if that’s your style, and &lt;a href=&#34;https://medium.com/ernesto-ramirez/38132ec3ac12&#34;&gt;Medium&lt;/a&gt; if you like that platform. Free free to connect with me on &lt;a href=&#34;https://twitter.com/eramirez&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;mailto:er.ramirez@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;The Basis B1 watch/band has since been released. I was lucky enough to get one and have enjoyed wearing it, especially after the new update. I&amp;rsquo;m still waiting on an API&amp;hellip;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;She originally bought a Fitbit One, but lost it pretty quickly. Her sister then bought her a Fitbit Zip that she&amp;rsquo;s been wearing it every day for over a year. She loves it.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;This was an easy prediction to make. We&amp;rsquo;ve already seen some great entries in the wearable sensing clothing from &lt;a href=&#34;http://www.hexoskin.com/en&#34;&gt;Hexoskin&lt;/a&gt;, &lt;a href=&#34;http://www.omsignal.com/&#34;&gt;OMsignal&lt;/a&gt;, and others.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Maybe I&amp;rsquo;m wrong about this given the current ecosystem of Bluetooth Low Energy devices, but this still kind of irks me. I originally wrote this when I was a heavy Garmin Forerunner user. I went on runs without my phone, which I don&amp;rsquo;t do now, and wanted my GPS watch to automagically upload that data. I still want that, but I&amp;rsquo;m in a shrinking minority and I&amp;rsquo;m probably wrong.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;I was still full on in my PhD program and this is the most  pretentious phrase in this piece. I&amp;rsquo;m leaving it in to remind me that I am not  a behavioral scientist. I am merely an observer.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;No surprise that Jawbone acquired Massive Health and has made some amazing strides in their mobile app design.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Seriously look at them. They have a huge market share in the activity tracking sector: &lt;a href=&#34;http://mobihealthnews.com/28825/fitbit-jawbone-nike-had-97-percent-of-fitness-tracker-retail-sales-in-2013/&#34;&gt;67%&lt;/a&gt;. Even with the Force recalls they&amp;rsquo;re the dominant player. Do you hear people saying they are building the Fuelband for cars? I don&amp;rsquo;t.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Data Dilemma</title>
      <link>/post/2014/03/11/the-data-dilemma/</link>
      <pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014/03/11/the-data-dilemma/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;It’s easier than every to measure anything we want, and it’s easier than ever to analyze that data, which means &lt;em&gt;no field of human endeavor is safe from the effects of big data&lt;/em&gt;.      -Derrick Harris, &lt;a href=&#34;http://gigaom.com/2014/03/10/an-mlb-team-is-apparently-doing-in-game-graph-analysis/&#34;&gt;GigOM&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you&amp;rsquo;re like me you think this is absolutely the best time to be alive. Our current state of technological progress appears to be at an all time high. We&amp;rsquo;ve been able to create systems for &lt;a href=&#34;http://blog.safecast.org/&#34;&gt;measuring the radiation&lt;/a&gt; after a nuclear accident with open source personal Geiger counters. We&amp;rsquo;re also able to strap a &lt;a href=&#34;http://www.mc10inc.com/consumer-products/sports/checklight/&#34;&gt;simple set of sensors&lt;/a&gt; to a child&amp;rsquo;s head in order to better track and understand head injuries in contact sports. These examples &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; are just two of the many that show how data is being intertwined with how we live our lives. This is no more apparent than on the field of professional sports.&lt;/p&gt;

&lt;p&gt;With big money at stake it&amp;rsquo;s no surprise that professional leagues and individual teams are fully embracing the big data craze. Anything and everything &amp;ldquo;for the win&amp;rdquo; has been true for centuries. Now, it&amp;rsquo;s come to include full-time statisticians, data analysts, and, in rare cases (for now), a supercomputer.&lt;/p&gt;

&lt;p&gt;The opening quote atop this piece comes from a recent &lt;a href=&#34;http://gigaom.com/2014/03/10/an-mlb-team-is-apparently-doing-in-game-graph-analysis/&#34;&gt;GigOM article&lt;/a&gt; based on on a piece published by the &lt;a href=&#34;http://www.economist.com/blogs/babbage/2014/03/supercomputers&#34;&gt;Economist&lt;/a&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; that describes a recent purchase of a &lt;a href=&#34;http://www.cray.com/Products/BigData/uRiKA.aspx&#34;&gt;Cray YarcData Urika Data Graph Appliance&lt;/a&gt; by an unnamed Major League Baseball team. From the &lt;a href=&#34;http://www.yarcdata.com/files/product-brief/Urika%20Product%20Brief.pdf&#34;&gt;product brief&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Urika graph analytics appliance from YarcData is purpose-built to meet these challenging requirements, transforming massive amounts of seemingly unrelated data into relevant insights. [&amp;hellip;] Urika can discover hidden relationships and unknown patterns in Big Data, do it with an unmatched level of speed and simplicity, and facilitate the kinds of breakthroughs that can give your enterprise a measurable competitive advantage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why would a baseball team want to have this machine somewhere in the recesses of their stadium? To do real-time in-game analysis of the players and the game. I have no doubt that the unnamed buyer is one of the three teams who have partnered with MLB Advanced Media to install &lt;a href=&#34;http://regressing.deadspin.com/mlb-announces-revolutionary-new-fielding-tracking-syste-1534200504&#34;&gt;a system of high speed cameras&lt;/a&gt; to track, &amp;ldquo;the speed and efficiency of fielders, based on highly accurate readings on hit balls—batted ball speed, launch angle, distance, hang time—and then how fast and how well the defenders react, capturing 30 frames per second on players and 2000 fps on the ball.&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; Imagine if you will, a manager being able to interact with live information about the opposing hitter and his history with the current pitcher. What pitches does he typically hit? Miss? Where is he likely to hit a slider with one man on? How far should the left fielder shade towards center? Five feet? Six inches?&lt;/p&gt;

&lt;p&gt;This is the near future of baseball, and more broadly all of professional sports. And to paraphrase Robert Frost, &amp;ldquo;that is making all the difference.&amp;rdquo; A difference that not everyone is comfortable with.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;What was more difficult for me to grasp was the way that the business of entertainment had really shifted the game and the sport of football in the NFL. The culture of football now is very different from the one I grew up with. When I came up, teammates fought together for wins and got respect for the fight. The player who gave the ball to the referee after a touchdown was commended; the one who played through injury was tough; the role of the blocking tight end was acknowledged; running backs who picked up blitzing linebackers showed heart; &lt;em&gt;and the story of the game was told through the tape, and not the stats alone&lt;/em&gt;. That was my model of football. -Rashard Mendehall, &lt;a href=&#34;http://www.huffingtonpost.com/rashard-mendenhall/rashard-mendenhall-retirement_b_4931316.html&#34;&gt;Huffington Post&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;rsquo;ve seen Moneyball a few times and I&amp;rsquo;ve always enjoyed it. Sure, it&amp;rsquo;s a only movie &lt;em&gt;based&lt;/em&gt; on true events and real people, but I think it&amp;rsquo;s a good example here for the dilemma that&amp;rsquo;s arose due to the creeping in of data in sports. There are a few pivotal scenes where Billy Beane, played wonderfully by Brad Pitt, is trying to make his case to scouts and the Atheltics&amp;rsquo; manager. They were having none of it. In their minds, data couldn&amp;rsquo;t match their experience and intuition, their &amp;ldquo;gut.&amp;rdquo; These fictional conversations are still ongoing out in the world today.&lt;/p&gt;

&lt;p&gt;Simply put, there is a perception that we&amp;rsquo;re losing the humanity by relinquishing control to the overlord of &amp;ldquo;big data.&amp;rdquo; Take Rashard Mendenhall who I quote above. He&amp;rsquo;s in the prime of his career at only 26 years old and playing for a good team. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; On March 9, 2014 he announced his retirement by writing a long piece for the Huffington Post. again, I&amp;rsquo;m probably cherry picking here, but it&amp;rsquo;s hard to mistake his words as a representation of the general feeling among many athletes and their fans. To them, stats and data are not sports, they&amp;rsquo;re cold hard numbers. These types feel that a reliance on data takes away from the joy and the magic of the game. That is reduces spontaneity, and the beauty of the unknown.&lt;/p&gt;

&lt;p&gt;I think they&amp;rsquo;re wrong.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This past weekend I was lucky to be a guest at a movie premiere. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; The movie, &lt;a href=&#34;http://personal-gold.com/&#34;&gt;Personal Gold&lt;/a&gt;, is a documentary about the 2012 women&amp;rsquo;s track cycling pursuit team that competed at the London Olympics. It was an incredible look at how hard four women pushed themselves to reach beyond their abilities and find that little bit more &lt;em&gt;something&lt;/em&gt;, that extra effort, extra inch, that makes all the difference. However, the movie wasn&amp;rsquo;t your typical Olympic heartwarming tale. A thread that ran through the entire picture was the story of how personal data and intensive physiological tracking helped the riders overcome a limited training staff and budget. Data on their sleep, sunlight exposure, genetic makeup, and their cycling power output was tracked, analyzed, and then used to tweak and fine tune every aspect of their training leading up to the Olympic games. Of course you can look up &lt;a href=&#34;http://www.usacycling.org/us-earns-silver-medal-in-womens-team-pursuit.htm&#34;&gt;the results&lt;/a&gt; to see what happened, or you can just believe me when I tell you that they pulled off something special.&lt;/p&gt;

&lt;p&gt;Data played a major part in their story and how they were able to overcome major adversity. However, data didn&amp;rsquo;t get on the bike and pedal it at over 30MPH for 3 minutes and 17 seconds. It  didn&amp;rsquo;t know, as Sara Hammer recalled during the Q&amp;amp;A after the screening, &amp;ldquo;[..] that I had more in me than I realized. That&amp;rsquo;s what being on a true team is like, having people like Dotsie who know you better than you know yourself.&amp;rdquo;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Humans are funny creatures. We have this massive amazing brain that can invent things like calculus and the Curiosity Rover. But for some reason we take our inventions, like numbers, and label them as inhuman. Cold. Mechanical. But they&amp;rsquo;re just a part of us as anything else we&amp;rsquo;ve birthed into existence with our minds.&lt;/p&gt;

&lt;p&gt;Yes, data is coming into all aspects of sports. Soon we&amp;rsquo;ll be able to watch a football game and see live prediction calculations on who should win. We&amp;rsquo;ll read more articles that are more data visualization than play-by-play reporting. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; ESPN has even invested in stats and analysis wundkind, Nate Silver. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; And baseball will lead the way, as it always has, into a brave new world of analytics. Does this mean that we&amp;rsquo;ll enjoy it less? See fewer amazing feats of strength and skill? Will be cease to be witness to the beautiful and the inspiring from stadiums, fields, and arenas around the world? No. I think not.&lt;/p&gt;

&lt;p&gt;In the end we&amp;rsquo;re still human. We&amp;rsquo;re prone to mistakes &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; and lapses in judgement. We&amp;rsquo;re not perfect machines no matter how many supercomputers we have hidden in our closets. No matter how many high speed cameras are watching a player there is always that chance that they might jump that extra inch a snag that would be home run that had a 100% chance of leaving the field. We haven&amp;rsquo;t yet lost our humanity, our ability to improvise and reach new unknowns, and do what sports does best, inspire awe.&lt;/p&gt;

&lt;p&gt;That is, until the robots enter the draft.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As always, comments are welcome. This post is available &lt;a href=&#34;https://github.com/erramirez/erramirez.github.io/blob/master/_posts/2014-03-11-the-data-dilemma.md&#34;&gt;on Github&lt;/a&gt; if that’s your style, and &lt;a href=&#34;https://medium.com/p/dd2faba3c00e&#34;&gt;Medium&lt;/a&gt; if you like that platform. Free free to connect with me on &lt;a href=&#34;https://twitter.com/eramirez&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;mailto:er.ramirez@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Cherry-picked, but hey it&amp;rsquo;s my article and I can do what I want.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Welcome to blogging in 2014, but I guess in the end I&amp;rsquo;m no different.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Even with all the negatives associated with Deadspin and the Gawker Empire I&amp;rsquo;m really enjoying their &lt;a href=&#34;http://regressing.deadspin.com/&#34;&gt;Regressing feature section&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Full disclosure: I grew up in Arizona and consider myself a Cardinals fan. Regardless, they are a &lt;em&gt;good&lt;/em&gt; team.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;I am so LA.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;The New York Times is already &lt;a href=&#34;http://www.nytimes.com/interactive/2012/06/11/sports/basketball/nba-shot-analysis.html&#34;&gt;all over this.&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Five Thiry Eight launches March 17th. Can you tell I&amp;rsquo;m excited?
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;And isn&amp;rsquo;t random error what makes statistics fun anyway?
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Living With Data</title>
      <link>/post/2014/03/02/living-with-data/</link>
      <pubDate>Sun, 02 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014/03/02/living-with-data/</guid>
      <description>&lt;p&gt;Earlier this week an old friend of mine sent me a message outlining a request. In an effort to get back to his habit of drawing and creating he was reaching to a few of his friends for some inspiration and ideas.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Simple premise. I&amp;rsquo;m inviting important people that I admire, respect, and care about to gift me one creative brief—a specific, attainable objective—to complete by my next birthday.
Example: Some sort of insight as to why people do something, don&amp;rsquo;t do something, or another aspect of human behavior. Think, &amp;ldquo;I want x because y&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What follow is slightly edited version of my response. I&amp;rsquo;m sharing it here because a) I too want to be more productive as well b) I want to practice writing and publicly exploring my thoughts and c) sharing is fun.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As you know, I&amp;rsquo;m very interested and engaged with what it means to live in a world of near ubiquitous personal data. The Quantified Self movement is a key piece of this and we&amp;rsquo;ve seen some amazing work by individuals and institutions (commercial and not). We&amp;rsquo;ve observed that the power that drives real insights can be directly traced to informative data visualizations. However, I feel that we are still in the infancy of what it means to really live with personal data. See &lt;a href=&#34;http://visualized.com/2014/presents/lev-manovich/&#34;&gt;this talk&lt;/a&gt; by Lev Manovich to see how little our data visualization techniques have changed.) (Did you know the bar chart has been around since 1778?)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to challenge you to explore what it means to Live &lt;em&gt;with&lt;/em&gt; Data. The conversation around QS typically centers around the reduction in autonomy and a move towards algorithmically driven lives (e.g. computers telling us what to do and when to do it based on our personal data). I’ve seen this expressed as Living By Numbers (Data). However, I feel this misses a big piece of the cultural shift. What happens when we Live &lt;em&gt;With&lt;/em&gt; Data as an piece of our human experience? (Some might say that the difference between &amp;ldquo;by&amp;rdquo; and &amp;ldquo;with&amp;rdquo; isn&amp;rsquo;t meaningful, or I&amp;rsquo;m being pedantic. Words have meaning and how we use them is important.) Data can live, breath, and communicate (listen and talk) with us. But, what form does that take?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to challenge you to explore this idea. Art and creative design can be a big influence on this concept. Here are a few examples of ideas I&amp;rsquo;ve been thinking of lately:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What if the walls in your home were made of LEDs or screens that could reflect your current mood? Would you want it to reflect or change. How? (Thanks to Laurie Frick for prompting this idea during a conversation in her home studio. You should check out her QS based art &lt;a href=&#34;http://www.lauriefrick.com/&#34;&gt;here&lt;/a&gt;.)&lt;/li&gt;
&lt;li&gt;What would it be like if you had physical manifestations of your data in your home or place of business. See &lt;a href=&#34;https://www.newschallenge.org/challenge/healthdata/evaluation/data-experiences-human-scale-3d-bar-graphs-for-environmental-health-data&#34;&gt;this project&lt;/a&gt; from MIT for inspiration.&lt;/li&gt;
&lt;li&gt;Data is typically represented as a direct manifestation of numerical information. Explore ideas of data abstraction. Data as art that tells you a story. This has been explored before with techniques like &lt;a href=&#34;http://en.wikipedia.org/wiki/Chernoff_face&#34;&gt;Chernoff Faces&lt;/a&gt; and more recently with Chloe Fan&amp;rsquo;s &lt;a href=&#34;http://www.sparkvis.com/&#34;&gt;Fitbit Spark visualization&lt;/a&gt;.What if we went further, more abstract and more interactive?&lt;/li&gt;
&lt;li&gt;This one is a little out there so proceed with caution. In the near future robotics and AI might be advanced enough to create actual human replicas or clones. Imagine this happens. What would it be like to create a clone of yourself and stream your personal data to it? Would you test experiments? Would you &lt;a href=&#34;http://en.wikipedia.org/wiki/Agent-based_model&#34;&gt;watch&lt;/a&gt; it (them?) try and learn new things? What data would you not share?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’ve read this far I’d love to hear what you think. What are the ideas that come to mind when you hear “Living With Data?” I look forward to hearing your thoughts. &lt;/p&gt;

&lt;p&gt;As always, comments are welcome. This post is available &lt;a href=&#34;https://github.com/erramirez/erramirez.github.io/blob/master/_posts/2014-03-02-living-with-data.md&#34;&gt;on Github&lt;/a&gt; if that’s your style, and &lt;a href=&#34;https://medium.com/p/f577b6a8411e&#34;&gt;Medium&lt;/a&gt; if you like that platform. Free free to connect with me on &lt;a href=&#34;https://twitter.com/eramirez&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;mailto:er.ramirez@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>